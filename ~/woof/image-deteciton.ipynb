{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname,_,filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(filename)\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision \nfrom torchvision import datasets, transforms, models\nfrom torch import optim as optim\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nimport matplotlib\nimport matplotlib.patches as patches\nimport glob\nimport xml.etree.ElementTree as ET\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = datasets.ImageFolder(\"../input/stanford-dogs-dataset/images/Images/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = list(sorted(os.listdir(\"../input/stanford-dogs-dataset/images/Images/\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_annotations = []\nlist_images = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l in label:\n    labels_annotation = list(sorted(os.listdir(os.path.join(\"../input/stanford-dogs-dataset/annotations/Annotation\", l))))\n    images = list(sorted(os.listdir(os.path.join(\"../input/stanford-dogs-dataset/images/Images\", l))))\n    list_annotations += labels_annotation\n    list_images += images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dogs(torch.utils.data.Dataset):\n    def __init__(self, dataset, transform):\n        self.img_path = \"../input/stanford-dogs-dataset/images/Images/\"\n        self.annotation_path = \"../input/stanford-dogs-dataset/annotations/Annotation/\"\n        self.dataset = dataset\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        image, type_  = self.dataset[idx]\n \n        image = cv2.imread(self.img_path + label[type_] + \"/\" + list_images[idx], cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n#         image = cv2.resize(image, (256, 256), interpolation = cv2.INTER_AREA)\n        image /= 255.\n        \n        image_id = torch.tensor([idx])\n#         label_image = torch.as_tensor(type_, dtype=torch.int64).view(-1,)\n        label_image = torch.ones(1, dtype=torch.int64)\n        \n        tree = ET.parse(self.annotation_path + label[type_] + \"/\" + list_annotations[idx])\n        root = tree.getroot()\n        boxes = []\n        value = []\n        for x in root[5][4][:]:\n            value.append(int(x.text))\n        boxes.append(value)\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            \n        area = (boxes[0][3]-boxes[0][1])*(boxes[0][2]-boxes[0][0])\n        area = torch.as_tensor(area, dtype=torch.float32).view(-1, )\n        \n        iscrowd = torch.zeros((1, ), dtype=torch.int64)\n        \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = label_image\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        \n        if self.transform is not None:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': label_image\n            }\n            sample = self.transform(**sample)\n            image = sample['image']\n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n            target['boxes'] = target['boxes'].float()\n        \n        return image, target\n    \n    \n    def __len__(self):\n        return len(self.dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Albumentations\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        A.Resize(256, 256),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 't', 'label_fields': ['labels']})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dog_dataset = Dogs(dataset, get_train_transform())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs = dog_dataset.__getitem__(1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.imshow(rs[0].numpy().transpose(1, 2,0))\nrect = patches.Rectangle((rs[1][\"boxes\"][0][0], rs[1][\"boxes\"][0][1]), (rs[1][\"boxes\"][0][2]-rs[1][\"boxes\"][0][0]), (rs[1][\"boxes\"][0][3]-rs[1][\"boxes\"][0][1]), linewidth=2, edgecolor=\"r\", facecolor='none')\nax.add_patch(rect)\nax.text(rs[1][\"boxes\"][0][0], rs[1][\"boxes\"][0][1]-5, label[rs[1][\"labels\"]], color=\"r\", fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_collate(batch):\n    return tuple(zip(*batch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_loader = torch.utils.data.DataLoader(dog_dataset, batch_size=32, shuffle=True, collate_fn=my_collate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"itr = iter(dt_loader)\nimgs, targets = next(itr)\nimages = list(image for image in imgs)\ntargets = [{k: v for k, v in t.items()} for t in targets]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n# backbone.out_channels = 1280\n\n# backbone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# anchor_generator = torchvision.models.detection.rpn.AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n#                                                                    aspect_ratios=((0.5, 1.0, 2.0),))\n# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0], output_size=7, sampling_ratio=2)\n# model = models.detection.FasterRCNN(backbone, num_classes=2, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(image, model, device, detection_threshold):\n    image = transform(image).to(device)\n    image = image.unsqueeze(0)\n    out = model(image)\n    \n    pred_classes = [coco_names[i] for i in out[0]['labels'].cpu().numpy()]\n    \n    pred_scores = out[0]['scores'].detach().cpu().numpy()\n    pred_bboxes = out[0]['boxes'].detach().cpu().numpy()\n    \n    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n    \n    print(out)\n    \n    return boxes, pred_classes, out[0]['labels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_bboxes(boxes, classes, labels, image):\n#     image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n#     image = image.cpu()\n    fig, ax = plt.subplots()\n    ax.imshow(image)\n    for i, box in enumerate(boxes):\n        color = COLORS[labels[i]]\n        rect = patches.Rectangle((box[0], box[1]), (box[2]-box[0]), (box[3]-box[1]), linewidth=2, edgecolor=color, facecolor='none')\n        ax.add_patch(rect)\n        ax.text(box[0], box[1]-5, classes[i], color=color, fontsize=10)\n#         cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)\n#         cv2.putText(image, classes[i], (int(box[0]), int(box[1]-5)),\n#                     cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n#                     lineType=cv2.LINE_AA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# num_epochs = 2\n# loss_hist = Averager()\n# model = model.to(device)\n# itr = 1\n# from torch import optim\n\n# params = [p for p in model.parameters() if p.requires_grad]\n# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n# for epoch in range(num_epochs):\n#     for images, targets in dt_loader:\n#         images = list(image.to(device) for image in images)\n#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n#         loss_dict = model(images, targets)\n#         losses = sum(loss for loss in loss_dict.values())\n#         loss_value = losses.item()\n#         loss_hist.send(loss_value)\n#         optimizer.zero_grad()\n#         losses.backward()\n#         optimizer.step()\n#         if itr % 50 == 0:\n#             print(f\"Iteration #{itr} loss: {loss_value}\")\n#         itr += 1\n#     # update the learning rate\n#     if lr_scheduler is not None:\n#         lr_scheduler.step()\n#     print(f\"Epoch #{epoch} loss: {loss_hist.value}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}